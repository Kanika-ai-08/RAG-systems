import os 
from langchain_community.document_loaders import TextLoader , DirectoryLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

load_dotenv()

#Ingestion Pipeline to create and persist vector store

def load_documents(docs_path = "docs"):
    if not os.path.exists(docs_path):
        raise ValueError(f"The specified path does not exist: {docs_path}")
    
    loader = DirectoryLoader(
        path=docs_path,
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )

    documents = loader.load()

    if len(documents) ==0:
       raise ValueError(f"No documents found in the specified path: {docs_path}")

    for i , doc in enumerate(documents[:2]):
      print(f"Document {i+1}: ") 
      print(f" Source: {doc.metadata['source']}")
      print(f"Content length: {len(doc.page_content)} characters ")
      print(f" Content preview: {doc.page_content[:100]}...")
      print(f" metadata: {doc.metadata}")

    return documents

def split_documents(documents, chunk_size=800, chunk_overlap=0):
    print("splitting documents into chunks...")

    text_splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator="\n"
    )
    chunks = text_splitter.split_documents(documents)

    if chunks:

        for i, chunk in enumerate(chunks[:5]):
            print(f"----------Chunk {i+1} ----------")
            print(f" Metadata: {chunk.metadata}")
            print(f" Content length: {len(chunk.page_content)} characters")
            print(f"Content:")
            print(chunk.page_content[:100])
            print("--"*50)

            if len(chunks) > 5 :
                print(f"... {len(chunks) - 5} more chunks ...")
               
    return chunks

def create_vector_store(chunks, persist_directory="db/chroma_db"):
    print("Creating vector store...")

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_metadata={"hnsw : space": "cosine"}
    )

    print(f"Vector store persisted at: {persist_directory}")

    return vector_store


def main():
    print("Main function started")
    documents = load_documents(docs_path="docs")
    chunks = split_documents(documents)
    vectorstore = create_vector_store(chunks)


if __name__ == "__main__":
    main()